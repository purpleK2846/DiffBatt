{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DiffBatt Model Testing and Review\n",
    "\n",
    "이 노트북에서는 DiffBatt 모델의 성능을 테스트하고 결과를 분석합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. testing.py와 동일한 코드 실행"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# DiffBatt 모델 테스트 실행\n# 이 셀은 testing.py와 동일한 코드를 실행합니다.\n# \n# 전체 프로세스:\n# 1. GPU 설정 및 메모리 관리\n# 2. 사전 학습된 모델 로드 (일반 네트워크 + EMA 네트워크)\n# 3. Gaussian Diffusion 유틸리티 초기화 (노이즈 추가/제거 과정 관리)\n# 4. DiffusionModel 생성 (실제 예측을 수행하는 모델)\n# 5. 테스트 데이터셋 로드\n# 6. PostProcess를 통한 예측 및 평가\n#    - 배터리 용량 매트릭스를 조건으로 SOH(State of Health) 곡선 생성\n#    - 노이즈에서 시작하여 1000단계의 역확산 과정을 통해 예측\n# 7. 평가 지표 계산 (SOH RMSE, RUL RMSE)\n# 8. 결과 시각화\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.data import Dataset as tfds\nfrom gaussian_diffusion import GaussianDiffusion\nfrom diffusion_model import DiffusionModel\nfrom postproc_utils import PostProcess\n\n# GPU 메모리 점진적 할당 설정\ngpus = tf.config.experimental.list_physical_devices('GPU')\nfor gpu in gpus:\n  tf.config.experimental.set_memory_growth(gpu, True)\n\ntf.config.set_visible_devices(gpus, 'GPU')\n\n# 설정 파라미터\nbattery_dataset = 'mix'  # 사용할 배터리 데이터셋\ntotal_timesteps = 1000   # 확산 과정의 총 단계 수\np_uncond = 0.2          # Classifier-free guidance를 위한 unconditional 확률\n\n# 사전 학습된 모델 로드\nmdir = './trained_models/'\nnetwork = keras.models.load_model(mdir+f'{battery_dataset}_network')\nema_network = keras.models.load_model(mdir+f'{battery_dataset}_ema_network')\n\n# Gaussian Diffusion 유틸리티 인스턴스 생성\ngdf_util = GaussianDiffusion(timesteps=total_timesteps)\n\n# DiffusionModel 인스턴스 생성\nmodel = DiffusionModel(\n    network=network,\n    ema_network=ema_network,\n    gdf_util=gdf_util,\n    timesteps=total_timesteps,\n    p_uncond=p_uncond,\n)\n\n# 학습 및 테스트 데이터셋 로드\ntrain_ds = tfds.load(f'./data/{battery_dataset}_train_ds')\ntest_ds = tfds.load(f'./data/{battery_dataset}_test_ds')\n\n# 예측 및 평가 수행\npost_process = PostProcess(test_ds, model)\nrefs, preds = post_process.pred(reps=1)  # reps=1: 각 샘플당 1번의 예측\nsoh_rmse = post_process.eval_soh(refs, preds)\nrul_rmse = post_process.eval_rul(refs, preds)\n\n# 결과 시각화 및 출력\npost_process.plot_sample(refs, preds)\nprint('RUL RMSE', rul_rmse, 'SOH RMSE', soh_rmse)"
  },
  {
   "cell_type": "code",
   "source": "# DiffBatt 모델의 상세 예측 과정 시각화\n# 이 셀은 .py 파일들의 내부 구현을 모두 포함하여 전체 예측 과정을 보여줍니다.\n\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nfrom tensorflow.data import Dataset as tfds\n\n# 1. 테스트 데이터셋 로드 및 전처리\nprint(\"1. 테스트 데이터셋 로드 중...\")\nbattery_dataset = 'mix'\ntest_ds = tfds.load(f'./data/{battery_dataset}_test_ds')\n\n# 데이터셋에서 SOH 참조값과 용량 매트릭스 추출\nrefs_soh = []\ncapacity_matrices = []\n\nfor data in test_ds.as_numpy_iterator():\n    refs_soh.append(data[0])  # SOH curves\n    capacity_matrices.append(data[2])  # Capacity matrices\n\nrefs_soh = np.array(refs_soh)\ncapacity_matrices = np.array(capacity_matrices)\nprint(f\"   - 로드된 테스트 샘플 수: {len(refs_soh)}\")\nprint(f\"   - SOH 데이터 shape: {refs_soh.shape}\")\nprint(f\"   - 용량 매트릭스 shape: {capacity_matrices.shape}\")\n\n# 2. 모델 로드\nprint(\"\\n2. 사전 학습된 모델 로드 중...\")\nmdir = './trained_models/'\nnetwork = keras.models.load_model(mdir+f'{battery_dataset}_network')\nema_network = keras.models.load_model(mdir+f'{battery_dataset}_ema_network')\nprint(\"   - 일반 네트워크와 EMA 네트워크 로드 완료\")\n\n# 3. Gaussian Diffusion 설정\nprint(\"\\n3. Gaussian Diffusion 설정...\")\ntotal_timesteps = 1000\n\n# Beta schedule 생성 (노이즈 스케줄)\ndef cosine_beta_schedule(timesteps, s=0.008):\n    steps = timesteps + 1\n    x = tf.linspace(0, timesteps, steps)\n    alphas_cumprod = tf.cos(((x / timesteps) + s) / (1 + s) * np.pi * 0.5) ** 2\n    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n    return tf.clip_by_value(betas, 0.0001, 0.9999)\n\nbetas = cosine_beta_schedule(total_timesteps)\nalphas = 1.0 - betas\nalphas_cumprod = tf.math.cumprod(alphas, axis=0)\nalphas_cumprod_prev = tf.concat([tf.ones((1,)), alphas_cumprod[:-1]], axis=0)\n\n# 확산 과정에 필요한 파라미터들\nsqrt_alphas_cumprod = tf.sqrt(alphas_cumprod)\nsqrt_one_minus_alphas_cumprod = tf.sqrt(1.0 - alphas_cumprod)\nsqrt_recip_alphas = tf.sqrt(1.0 / alphas)\nposterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n\nprint(f\"   - Beta schedule 생성 완료 (timesteps: {total_timesteps})\")\n\n# 4. 역확산 과정 구현 (Denoising)\nprint(\"\\n4. 예측 시작 (역확산 과정)...\")\n\n# 첫 번째 테스트 샘플로 시연\nsample_idx = 0\nsample_capacity = capacity_matrices[sample_idx:sample_idx+1]\n\n# 랜덤 노이즈에서 시작\nimage_size = 64\nchannels = 1\nnoise = tf.random.normal(shape=(1, image_size, image_size, channels))\nsample = noise\n\n# 시각화를 위한 중간 단계 저장\nintermediate_steps = []\nsave_steps = [999, 800, 600, 400, 200, 100, 50, 0]\n\nprint(\"   - 역확산 시작 (1000 → 0 timesteps)\")\nfor t in reversed(range(0, total_timesteps)):\n    # 현재 timestep\n    tt = tf.cast(tf.fill(1, t), dtype=tf.int64)\n    \n    # 모델을 통한 노이즈 예측\n    pred_noise = ema_network([sample, tt, sample_capacity], training=False)\n    \n    # 예측된 원본 이미지 계산\n    coeff1 = tf.cast(tf.reshape(1.0 / sqrt_alphas_cumprod[t], [1, 1, 1, 1]), sample.dtype)\n    coeff2 = tf.cast(tf.reshape(sqrt_one_minus_alphas_cumprod[t], [1, 1, 1, 1]), sample.dtype)\n    pred_x0 = coeff1 * (sample - coeff2 * pred_noise)\n    pred_x0 = tf.clip_by_value(pred_x0, -1.0, 1.0)\n    \n    # 평균과 분산 계산\n    coeff1 = tf.reshape(sqrt_recip_alphas[t], [1, 1, 1, 1])\n    coeff2 = tf.reshape(betas[t] / sqrt_one_minus_alphas_cumprod[t], [1, 1, 1, 1])\n    model_mean = coeff1 * (sample - coeff2 * pred_noise)\n    \n    # 다음 샘플 생성\n    if t > 0:\n        noise = tf.random.normal(shape=sample.shape)\n        nonzero_mask = tf.cast(tf.reshape(1 - (t == 0), [1, 1, 1, 1]), sample.dtype)\n        std_dev = tf.sqrt(posterior_variance[t])\n        sample = model_mean + nonzero_mask * tf.reshape(std_dev, [1, 1, 1, 1]) * noise\n    else:\n        sample = model_mean\n    \n    # 중간 단계 저장\n    if t in save_steps:\n        intermediate_steps.append((t, sample.numpy().copy()))\n\nprint(\"   - 역확산 완료\")\n\n# 5. 후처리 및 SOH 곡선 변환\nprint(\"\\n5. 예측 결과 후처리...\")\n\n# 최종 예측 이미지를 SOH 곡선으로 변환\nfinal_prediction = sample.numpy()\n\n# 이미지를 1차원 SOH 시계열로 변환 (reshape)\npred_soh = final_prediction.reshape(-1, 64*64)\n\n# 스케일 조정 (이미지 값 [-1, 1] → SOH [0, 100])\npred_soh = (pred_soh + 1) * 50  # -1~1 → 0~100\n\n# 2560 사이클로 리사이즈\nfrom scipy.interpolate import interp1d\nx_old = np.linspace(0, 1, pred_soh.shape[1])\nx_new = np.linspace(0, 1, 2560)\nf = interp1d(x_old, pred_soh[0], kind='linear')\npred_soh_resized = f(x_new)\n\n# 참조 SOH도 리사이즈\nref_soh = refs_soh[sample_idx]\nf_ref = interp1d(np.linspace(0, 1, len(ref_soh)), ref_soh, kind='linear')\nref_soh_resized = f_ref(x_new)\n\nprint(f\"   - 예측 SOH shape: {pred_soh_resized.shape}\")\n\n# 6. 평가 지표 계산\nprint(\"\\n6. 평가 지표 계산...\")\n\n# SOH RMSE (60% 이상만 계산)\nmask = ref_soh_resized > 60\nsoh_rmse = np.sqrt(np.mean((ref_soh_resized[mask] - pred_soh_resized[mask])**2))\n\n# RUL 계산 (80% 임계값)\ndef get_rul(soh_curve, eol=80):\n    indices = np.where(soh_curve < eol)[0]\n    return indices[0] if len(indices) > 0 else len(soh_curve)\n\nref_rul = get_rul(ref_soh_resized)\npred_rul = get_rul(pred_soh_resized)\nrul_error = abs(ref_rul - pred_rul)\n\nprint(f\"   - SOH RMSE: {soh_rmse:.4f}\")\nprint(f\"   - 참조 RUL: {ref_rul} cycles\")\nprint(f\"   - 예측 RUL: {pred_rul} cycles\")\nprint(f\"   - RUL 오차: {rul_error} cycles\")\n\n# 7. 시각화\nprint(\"\\n7. 결과 시각화...\")\n\nfig, axes = plt.subplots(2, 4, figsize=(16, 8))\n\n# 역확산 과정 시각화\nfor i, (t, img) in enumerate(intermediate_steps):\n    ax = axes[0, i]\n    ax.imshow(img[0, :, :, 0], cmap='viridis')\n    ax.set_title(f'Timestep {t}')\n    ax.axis('off')\n\n# SOH 예측 결과\nax = axes[1, :2].flatten()[0]\nax.plot(ref_soh_resized, label='Reference', alpha=0.7)\nax.plot(pred_soh_resized, label='Prediction', alpha=0.7)\nax.axhline(y=80, color='r', linestyle='--', alpha=0.5, label='EOL (80%)')\nax.set_xlabel('Cycles')\nax.set_ylabel('SOH (%)')\nax.set_title('SOH Prediction vs Reference')\nax.legend()\nax.grid(True, alpha=0.3)\n\n# 오차 분석\nax = axes[1, 2]\nerror = ref_soh_resized - pred_soh_resized\nax.plot(error)\nax.axhline(y=0, color='k', linestyle='-', alpha=0.5)\nax.set_xlabel('Cycles')\nax.set_ylabel('Error (%)')\nax.set_title('Prediction Error')\nax.grid(True, alpha=0.3)\n\n# 용량 매트릭스 시각화\nax = axes[1, 3]\nax.imshow(sample_capacity[0, :, :, 0], cmap='hot')\nax.set_title('Capacity Matrix (Condition)')\nax.axis('off')\n\nplt.suptitle('DiffBatt Model: 전체 예측 과정 시각화', fontsize=16)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n전체 예측 과정 완료!\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 다른 데이터셋 테스트하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용 가능한 데이터셋들\n",
    "available_datasets = ['mix', 'clo', 'cruh', 'crush', 'matr_1', 'matr_2', 'snl']\n",
    "\n",
    "def test_other_dataset(dataset_name):\n",
    "    \"\"\"다른 데이터셋으로 테스트\"\"\"\n",
    "    print(f\"\\nTesting {dataset_name} dataset...\")\n",
    "    \n",
    "    # 모델 로드\n",
    "    network = keras.models.load_model(f'./trained_models/{dataset_name}_network')\n",
    "    ema_network = keras.models.load_model(f'./trained_models/{dataset_name}_ema_network')\n",
    "    \n",
    "    # 모델 생성\n",
    "    model = DiffusionModel(\n",
    "        network=network,\n",
    "        ema_network=ema_network,\n",
    "        gdf_util=gdf_util,\n",
    "        timesteps=total_timesteps,\n",
    "        p_uncond=p_uncond,\n",
    "    )\n",
    "    \n",
    "    # 데이터셋 로드\n",
    "    test_ds = tfds.load(f'./data/{dataset_name}_test_ds')\n",
    "    \n",
    "    # 예측 및 평가\n",
    "    post_process = PostProcess(test_ds, model)\n",
    "    refs, preds = post_process.pred(reps=1)\n",
    "    soh_rmse = post_process.eval_soh(refs, preds)\n",
    "    rul_rmse = post_process.eval_rul(refs, preds)\n",
    "    \n",
    "    post_process.plot_sample(refs, preds)\n",
    "    print(f'{dataset_name.upper()} - RUL RMSE: {rul_rmse:.4f}, SOH RMSE: {soh_rmse:.4f}')\n",
    "    \n",
    "    return rul_rmse, soh_rmse\n",
    "\n",
    "print(\"Available datasets:\", available_datasets)\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"test_other_dataset('clo')\")\n",
    "print(\"test_other_dataset('snl')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CLO 데이터셋 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLO 데이터셋 테스트\n",
    "test_other_dataset('clo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SNL 데이터셋 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNL 데이터셋 테스트\n",
    "test_other_dataset('snl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MATR_1 데이터셋 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MATR_1 데이터셋 테스트\n",
    "test_other_dataset('matr_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 모든 데이터셋 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 데이터셋 성능 비교\n",
    "results = {}\n",
    "\n",
    "for dataset in available_datasets:\n",
    "    try:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Testing {dataset.upper()} Dataset\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        rul_rmse, soh_rmse = test_other_dataset(dataset)\n",
    "        results[dataset] = {'RUL_RMSE': rul_rmse, 'SOH_RMSE': soh_rmse}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing {dataset}: {e}\")\n",
    "        results[dataset] = {'RUL_RMSE': None, 'SOH_RMSE': None}\n",
    "\n",
    "# 결과 요약\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY OF ALL DATASETS PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Dataset':<12} {'RUL RMSE':<15} {'SOH RMSE':<15}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for dataset, metrics in results.items():\n",
    "    rul = f\"{metrics['RUL_RMSE']:.4f}\" if metrics['RUL_RMSE'] is not None else \"Error\"\n",
    "    soh = f\"{metrics['SOH_RMSE']:.4f}\" if metrics['SOH_RMSE'] is not None else \"Error\"\n",
    "    print(f\"{dataset.upper():<12} {rul:<15} {soh:<15}\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffbatt",
   "language": "python",
   "name": "diffbatt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}