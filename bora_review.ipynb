{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DiffBatt Model Testing and Review\n",
    "\n",
    "이 노트북에서는 DiffBatt 모델의 성능을 테스트하고 결과를 분석합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. testing.py와 동일한 코드 실행"
   ]
  },
  {
   "cell_type": "code",
   "source": "# TensorFlow 전역 데이터 타입 설정 - float32로 통일\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Keras 백엔드 float 타입을 float32로 설정\nkeras.backend.set_floatx('float32')\n\n# Mixed precision policy 설정 (선택사항)\npolicy = tf.keras.mixed_precision.Policy('float32')\ntf.keras.mixed_precision.set_global_policy(policy)\n\nprint(f\"Keras backend float type: {keras.backend.floatx()}\")\nprint(f\"TensorFlow global policy: {tf.keras.mixed_precision.global_policy().name}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 전역 TensorFlow 설정 - float32를 기본 데이터 타입으로 설정\nimport tensorflow as tf\n\n# 기본 float 타입을 float32로 설정\ntf.keras.backend.set_floatx('float32')\n\n# TensorFlow의 기본 dtype 정책 설정\nfrom tensorflow.keras import mixed_precision\npolicy = mixed_precision.Policy('float32')\nmixed_precision.set_global_policy(policy)\n\nprint(\"TensorFlow 기본 데이터 타입 설정:\")\nprint(f\"- Keras float 타입: {tf.keras.backend.floatx()}\")\nprint(f\"- Mixed precision policy: {mixed_precision.global_policy().name}\")\nprint(\"모든 연산이 float32로 수행됩니다.\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# DiffBatt 모델의 상세 예측 과정 시각화\n# 이 셀은 .py 파일들의 내부 구현을 모두 포함하여 전체 예측 과정을 보여줍니다.\n\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nfrom tensorflow.data import Dataset as tfds\n\n# 1. 테스트 데이터셋 로드 및 전처리\nprint(\"1. 테스트 데이터셋 로드 중...\")\nbattery_dataset = 'mix'\ntest_ds = tfds.load(f'./data/{battery_dataset}_test_ds')\n\n# 데이터셋에서 SOH 참조값과 용량 매트릭스 추출\nrefs_soh = []\ncapacity_matrices = []\n\nfor data in test_ds.as_numpy_iterator():\n    refs_soh.append(data[0].astype(np.float32))  # SOH curves - float32로 변환\n    capacity_matrices.append(data[2].astype(np.float32))  # Capacity matrices - float32로 변환\n\nrefs_soh = np.array(refs_soh, dtype=np.float32)\ncapacity_matrices = np.array(capacity_matrices, dtype=np.float32)\nprint(f\"   - 로드된 테스트 샘플 수: {len(refs_soh)}\")\nprint(f\"   - SOH 데이터 shape: {refs_soh.shape}\")\nprint(f\"   - 용량 매트릭스 shape: {capacity_matrices.shape}\")\n\n# 2. 모델 로드\nprint(\"\\n2. 사전 학습된 모델 로드 중...\")\nmdir = './trained_models/'\nnetwork = keras.models.load_model(mdir+f'{battery_dataset}_network')\nema_network = keras.models.load_model(mdir+f'{battery_dataset}_ema_network')\nprint(\"   - 일반 네트워크와 EMA 네트워크 로드 완료\")\n\n# 3. Gaussian Diffusion 설정\nprint(\"\\n3. Gaussian Diffusion 설정...\")\ntotal_timesteps = 1000\n\n# Beta schedule 생성 (노이즈 스케줄) - 모든 연산을 float32로\ndef cosine_beta_schedule(timesteps, s=0.008):\n    steps = timesteps + 1\n    x = tf.linspace(0.0, float(timesteps), steps)\n    alphas_cumprod = tf.cos(((x / timesteps) + s) / (1 + s) * tf.constant(np.pi, dtype=tf.float32) * 0.5) ** 2\n    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n    betas = 1.0 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n    return tf.clip_by_value(betas, 0.0001, 0.9999)\n\nbetas = cosine_beta_schedule(total_timesteps)\nalphas = 1.0 - betas\nalphas_cumprod = tf.math.cumprod(alphas, axis=0)\nalphas_cumprod_prev = tf.concat([tf.ones((1,), dtype=tf.float32), alphas_cumprod[:-1]], axis=0)\n\n# 확산 과정에 필요한 파라미터들 (모두 float32)\nsqrt_alphas_cumprod = tf.sqrt(alphas_cumprod)\nsqrt_one_minus_alphas_cumprod = tf.sqrt(1.0 - alphas_cumprod)\nsqrt_recip_alphas = tf.sqrt(1.0 / alphas)\nposterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n\nprint(f\"   - Beta schedule 생성 완료 (timesteps: {total_timesteps})\")\n\n# 4. 역확산 과정 구현 (Denoising)\nprint(\"\\n4. 예측 시작 (역확산 과정)...\")\n\n# 첫 번째 테스트 샘플로 시연\nsample_idx = 0\n\n# 올바른 형태로 capacity matrix 준비 - 개별 샘플 선택\nsample_capacity = tf.constant(capacity_matrices[sample_idx:sample_idx+1], dtype=tf.float32)\nprint(f\"   - 선택된 capacity matrix shape: {sample_capacity.shape}\")\n\n# 랜덤 노이즈에서 시작 - UNet 모델이 기대하는 형태는 (batch, 64, 64, 1)\nbatch_size = 1\nimage_size = 64\nchannels = 1\nnoise = tf.random.normal(shape=(batch_size, image_size, image_size, channels), dtype=tf.float32)\nsample = noise\n\n# 시각화를 위한 중간 단계 저장\nintermediate_steps = []\nsave_steps = [999, 800, 600, 400, 200, 100, 50, 0]\n\nprint(\"   - 역확산 시작 (1000 → 0 timesteps)\")\nfor t in reversed(range(0, total_timesteps)):\n    # 현재 timestep\n    tt = tf.constant([t], dtype=tf.int64)\n    \n    # 모델을 통한 노이즈 예측 - 3개의 입력 제공 (sample, timestep, capacity_matrix)\n    pred_noise = ema_network([sample, tt, sample_capacity], training=False)\n    \n    # 모든 계수를 float32로 확실하게 변환\n    sqrt_alpha_t = tf.cast(sqrt_alphas_cumprod[t], tf.float32)\n    sqrt_one_minus_alpha_t = tf.cast(sqrt_one_minus_alphas_cumprod[t], tf.float32)\n    \n    # 예측된 원본 데이터 계산\n    pred_x0 = (sample - sqrt_one_minus_alpha_t * pred_noise) / sqrt_alpha_t\n    pred_x0 = tf.clip_by_value(pred_x0, -1.0, 1.0)\n    \n    # 평균 계산\n    beta_t = tf.cast(betas[t], tf.float32)\n    sqrt_recip_alpha_t = tf.cast(sqrt_recip_alphas[t], tf.float32)\n    model_mean = sqrt_recip_alpha_t * (sample - (beta_t / sqrt_one_minus_alpha_t) * pred_noise)\n    \n    # 다음 샘플 생성\n    if t > 0:\n        noise = tf.random.normal(shape=sample.shape, dtype=tf.float32)\n        posterior_variance_t = tf.cast(posterior_variance[t], tf.float32)\n        sample = model_mean + tf.sqrt(posterior_variance_t) * noise\n    else:\n        sample = model_mean\n    \n    # 중간 단계 저장\n    if t in save_steps:\n        intermediate_steps.append((t, sample.numpy().copy()))\n\nprint(\"   - 역확산 완료\")\n\n# 5. 후처리 및 SOH 곡선 변환\nprint(\"\\n5. 예측 결과 후처리...\")\n\n# 최종 예측을 SOH 곡선으로 변환\nfinal_prediction = sample.numpy()  # shape: (1, 64, 64, 1)\n\n# 이미지를 1차원 SOH 시계열로 변환 (reshape)\npred_soh = final_prediction.reshape(-1, 64*64)  # shape: (1, 4096)\n\n# 스케일 조정 ([-1, 1] → [0, 100])\npred_soh = (pred_soh + 1) * 50  # -1~1 → 0~100\n\n# 256개 포인트로 다운샘플링 후 2560 사이클로 업샘플링 (PostProcess와 동일한 방식)\n# 먼저 256개 포인트로 다운샘플링\nindices = np.linspace(0, pred_soh.shape[1]-1, 256, dtype=int)\npred_soh_256 = pred_soh[0, indices]\n\n# 256개를 2560개로 업샘플링\npred_soh_reshaped = pred_soh_256.reshape(1, -1, 1)  # (1, 256, 1)\npred_soh_resized = tf.image.resize(pred_soh_reshaped, [2560, 1], method='nearest')\npred_soh_resized = pred_soh_resized.numpy().squeeze()  # (2560,)\n\n# 참조 SOH도 리사이즈\nref_soh = refs_soh[sample_idx]\nref_soh_reshaped = ref_soh.reshape(1, -1, 1)\nref_soh_resized = tf.image.resize(ref_soh_reshaped, [2560, 1], method='nearest')\nref_soh_resized = ref_soh_resized.numpy().squeeze()\n\nprint(f\"   - 예측 SOH shape: {pred_soh_resized.shape}\")\n\n# 6. 평가 지표 계산\nprint(\"\\n6. 평가 지표 계산...\")\n\n# SOH RMSE (60% 이상만 계산)\nmask = ref_soh_resized > 60\nsoh_rmse = np.sqrt(np.mean((ref_soh_resized[mask] - pred_soh_resized[mask])**2))\n\n# RUL 계산 (80% 임계값)\ndef get_rul(soh_curve, eol=80):\n    indices = np.where(soh_curve < eol)[0]\n    return indices[0] if len(indices) > 0 else len(soh_curve)\n\nref_rul = get_rul(ref_soh_resized)\npred_rul = get_rul(pred_soh_resized)\nrul_error = abs(ref_rul - pred_rul)\n\nprint(f\"   - SOH RMSE: {soh_rmse:.4f}\")\nprint(f\"   - 참조 RUL: {ref_rul} cycles\")\nprint(f\"   - 예측 RUL: {pred_rul} cycles\")\nprint(f\"   - RUL 오차: {rul_error} cycles\")\n\n# 7. 시각화\nprint(\"\\n7. 결과 시각화...\")\n\nfig, axes = plt.subplots(2, 4, figsize=(16, 8))\n\n# 역확산 과정 시각화\nfor i, (t, img) in enumerate(intermediate_steps):\n    ax = axes[0, i]\n    ax.imshow(img[0, :, :, 0], cmap='viridis', aspect='auto')\n    ax.set_title(f'Timestep {t}')\n    ax.axis('off')\n\n# SOH 예측 결과\nax = axes[1, 0]\nax.plot(ref_soh_resized, label='Reference', alpha=0.7)\nax.plot(pred_soh_resized, label='Prediction', alpha=0.7)\nax.axhline(y=80, color='r', linestyle='--', alpha=0.5, label='EOL (80%)')\nax.set_xlabel('Cycles')\nax.set_ylabel('SOH (%)')\nax.set_title('SOH Prediction vs Reference')\nax.legend()\nax.grid(True, alpha=0.3)\n\n# 오차 분석\nax = axes[1, 1]\nerror = ref_soh_resized - pred_soh_resized\nax.plot(error)\nax.axhline(y=0, color='k', linestyle='-', alpha=0.5)\nax.set_xlabel('Cycles')\nax.set_ylabel('Error (%)')\nax.set_title('Prediction Error')\nax.grid(True, alpha=0.3)\n\n# 원본 SOH 시계열 표시\nax = axes[1, 2]\nax.plot(refs_soh[sample_idx], 'b-', alpha=0.7)\nax.set_xlabel('Original Cycles')\nax.set_ylabel('SOH (%)')\nax.set_title(f'Original SOH Data ({len(refs_soh[sample_idx])} cycles)')\nax.grid(True, alpha=0.3)\n\n# 용량 매트릭스 시각화\nax = axes[1, 3]\nax.imshow(capacity_matrices[sample_idx, :, :, 0], cmap='hot', aspect='auto')\nax.set_title('Capacity Matrix (Condition)')\nax.set_xlabel('Features')\nax.set_ylabel('Time Steps')\n\nplt.suptitle('DiffBatt Model: 전체 예측 과정 시각화', fontsize=16)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n전체 예측 과정 완료!\")"
  },
  {
   "cell_type": "code",
   "source": "# 사용 가능한 데이터셋들\navailable_datasets = ['mix', 'clo', 'cruh', 'crush', 'matr_1', 'matr_2', 'snl']\n\ndef test_other_dataset(dataset_name):\n    \"\"\"다른 데이터셋으로 테스트\"\"\"\n    print(f\"\\nTesting {dataset_name} dataset...\")\n    \n    # 필요한 임포트\n    from gaussian_diffusion import GaussianDiffusion\n    \n    # 모델 로드\n    network = keras.models.load_model(f'./trained_models/{dataset_name}_network')\n    ema_network = keras.models.load_model(f'./trained_models/{dataset_name}_ema_network')\n    \n    # Gaussian Diffusion 유틸리티 생성\n    gdf_util = GaussianDiffusion(timesteps=total_timesteps)\n    \n    # 모델 생성\n    model = DiffusionModel(\n        network=network,\n        ema_network=ema_network,\n        gdf_util=gdf_util,\n        timesteps=total_timesteps,\n        p_uncond=p_uncond,\n    )\n    \n    # 데이터셋 로드\n    test_ds = tfds.load(f'./data/{dataset_name}_test_ds')\n    \n    # 예측 및 평가\n    post_process = PostProcess(test_ds, model)\n    refs, preds = post_process.pred(reps=1)\n    soh_rmse = post_process.eval_soh(refs, preds)\n    rul_rmse = post_process.eval_rul(refs, preds)\n    \n    post_process.plot_sample(refs, preds)\n    print(f'{dataset_name.upper()} - RUL RMSE: {rul_rmse:.4f}, SOH RMSE: {soh_rmse:.4f}')\n    \n    return rul_rmse, soh_rmse\n\nprint(\"Available datasets:\", available_datasets)\nprint(\"\\nExample usage:\")\nprint(\"test_other_dataset('clo')\")\nprint(\"test_other_dataset('snl')\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 다른 데이터셋 테스트하기"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 사용 가능한 데이터셋들\navailable_datasets = ['mix', 'clo', 'cruh', 'crush', 'matr_1', 'matr_2', 'snl']\n\ndef test_other_dataset(dataset_name):\n    \"\"\"다른 데이터셋으로 테스트 (float32 데이터 타입 보장)\"\"\"\n    print(f\"\\nTesting {dataset_name} dataset...\")\n    \n    # 모델 로드\n    network = keras.models.load_model(f'./trained_models/{dataset_name}_network')\n    ema_network = keras.models.load_model(f'./trained_models/{dataset_name}_ema_network')\n    \n    # GaussianDiffusion 인스턴스 생성 (float32 보장)\n    from gaussian_diffusion import GaussianDiffusion\n    gdf_util = GaussianDiffusion(timesteps=total_timesteps)\n    \n    # 모델 생성\n    model = DiffusionModel(\n        network=network,\n        ema_network=ema_network,\n        gdf_util=gdf_util,\n        timesteps=total_timesteps,\n        p_uncond=p_uncond,\n    )\n    \n    # 데이터셋 로드\n    test_ds = tfds.load(f'./data/{dataset_name}_test_ds')\n    \n    # 예측 및 평가\n    post_process = PostProcess(test_ds, model)\n    refs, preds = post_process.pred(reps=1)\n    soh_rmse = post_process.eval_soh(refs, preds)\n    rul_rmse = post_process.eval_rul(refs, preds)\n    \n    post_process.plot_sample(refs, preds)\n    print(f'{dataset_name.upper()} - RUL RMSE: {rul_rmse:.4f}, SOH RMSE: {soh_rmse:.4f}')\n    \n    return rul_rmse, soh_rmse\n\nprint(\"Available datasets:\", available_datasets)\nprint(\"\\nExample usage:\")\nprint(\"test_other_dataset('clo')\")\nprint(\"test_other_dataset('snl')\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CLO 데이터셋 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLO 데이터셋 테스트\n",
    "test_other_dataset('clo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SNL 데이터셋 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNL 데이터셋 테스트\n",
    "test_other_dataset('snl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MATR_1 데이터셋 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MATR_1 데이터셋 테스트\n",
    "test_other_dataset('matr_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 모든 데이터셋 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 데이터셋 성능 비교\n",
    "results = {}\n",
    "\n",
    "for dataset in available_datasets:\n",
    "    try:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Testing {dataset.upper()} Dataset\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        rul_rmse, soh_rmse = test_other_dataset(dataset)\n",
    "        results[dataset] = {'RUL_RMSE': rul_rmse, 'SOH_RMSE': soh_rmse}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing {dataset}: {e}\")\n",
    "        results[dataset] = {'RUL_RMSE': None, 'SOH_RMSE': None}\n",
    "\n",
    "# 결과 요약\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY OF ALL DATASETS PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Dataset':<12} {'RUL RMSE':<15} {'SOH RMSE':<15}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for dataset, metrics in results.items():\n",
    "    rul = f\"{metrics['RUL_RMSE']:.4f}\" if metrics['RUL_RMSE'] is not None else \"Error\"\n",
    "    soh = f\"{metrics['SOH_RMSE']:.4f}\" if metrics['SOH_RMSE'] is not None else \"Error\"\n",
    "    print(f\"{dataset.upper():<12} {rul:<15} {soh:<15}\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffbatt",
   "language": "python",
   "name": "diffbatt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}